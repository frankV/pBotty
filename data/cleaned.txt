Smalltalk is an object-oriented programming language which has been existence for several decades and was a major influence upon many popular modern programming languages. This paper explores some of the details involving the development and use of Smalltalk, and makes a judgement regarding the promotion of its use in the education and workspace arenas. Subject matter covered includes a brief overview of Smalltalk’s origin, its core themes, its functionality and usage and finally its applicability. Smalltalk is a mature programming language which fully embraces the object-oriented programming paradigm. Originally developed in the 1970s, Smalltalk-80 was the first publicly available version and is what most individuals who speak of Smalltalk in a historical context refer to. The current standard version of Smalltalk is ANSI Smalltalk, ratified in 1998 and is the version this paper focuses on.With the rise of the object-oriented paradigm, the representation of data types as objects was transitioning from a feature of languages to a focus. Simula 67 is a programming language which fully embraced the object-oriented paradigm and resulted in much of the commonly used terminology of modern day object-oriented programming such as classes and virtual methods. However, However, Simula still had primitive types which were not themselves objects. Smalltalk took the trend to the logical extreme and eliminated primitive types, implementing absolutely everything as an object. This includes built-in types, data structures, messages, functions and more. The radical no-holds-barred approach solidified the language’s place in history and served as inspiration for many future languages with an object-oriented focus such as C++, Java, Python, Ruby and many others. Smalltalk was a research product of the team headed by Alan Kay at Xerox Palo Alto Research Center. Inspiration came from a bet regarding the design of a language with immense power in a page of code. After some time, various design schemes manifested what would be dubbed Smalltalk-72. With the basic principles of the language in hand, the way was set for development and eventually release in the form of Smalltalk-80. Over the decades since its introduction into the programming ecosystem, many derivative versions have come to be. The more popular of these are the open-source Squeak, Pharo and GNU-Smalltalk, as well as the proprietary ObjectStudio and VisualWorks.There are many implementations of Smalltalk, though they by and large abide by the standards. Abstracting away primitive types and providing message passing as the underlying communication mechanism requires abstracting away communication with the machine itself. Thus Smalltalk code is largely executed through compiling to bytecode much like Java, and then utilizing a virtual machine or dynamically translating to machine code to run. Due to this requirement of intermediate steps, Smalltalk is naturally slower at accomplishing typical programming tasks. However, the design of an object-oriented program which efficiently manages messages may provide better functionality and sustainability than the equivalent program in a typical language not implementing object-oriented techniques.Variables are declared by encasing them in ‘|’ symbols, and must be declared before their use. Variables only exist within the scopes they were declared in. Assignment is done using the ‘:=’ operator. An operation is terminated with a period, but the last line of a method or function need not have the period as there are no further statements to separate it from within the block. Returns are stated using the caret symbol before a value or expression. Blocks of code are separated using brackets, and are themselves objects as well. All variables are instances of an object, and the name of a variable initially points to Nil before a value is assigned to the identifier. Conditionals are constructed by passing a boolean object the ifTrue and ifFalse messages with respective blocks of code to execute. Loops can be created using the times Repeat message, the whileTrue and whileFalse messages, or the to: and do: messages.Like the modern programming languages Ruby and Java, Smalltalk is reflective. In the context of a programming language, this means that it is capable of modifying its own code and functionality at runtime. This is accomplished thanks to the somewhat interpreted nature resulting from the aforementioned virtual machine and bytecode. Such a feature can lead to more responsive and dynamically composed software which can easily adapt to changing requirements or demands. Unfortunately this feature also poses serious security risks and may lead to undefined or unexpected behavior. In fact, Java implements security managers and the documentation condones excessive use of reflectivity.While the passing of messages as an analogue for function calls is not too difficult an adaptation to make for a programmer, the sheer number of messages create a massive learning curve. For any given class, it can be passed any imaginable message. The class may not recognize the message and throw an exception, but the primitive classes that ship with an implementation of Smalltalk can easily have on the order of a hundred accepted messages which resemble spoken English.For example, examine the Date class from earlier. It accepts the unary messages Today, Yesterday and asSeconds, the binary expressions < and =, the keyword expressions addDays: dayCount, fromDays: dayCount, newDay: day month: monthName year: yearInteger, and many more. To even an experienced programmer, learning all these conventional and built-in methods can be a daunting task. For a novice programmer just learning how to write functional code, the unintuitive nature of smalltalk can be problematic. In terms of the ease of use, rapid production capability and existing support, it is not as relevant as C/C++ and Java in modern software companies.Smalltalk enjoys a thriving and growing community which continually produces interesting content and boasts the merits of object-oriented programming. The language itself is highly featured, supporting common objects such as lists, and strings in addition to being reflective. As a learning experience for message passing in programming of systems or software, or as a conceptual and design practice for object-oriented programming, I highly recommend Smalltalk. However, I do not recommend Smalltalk as an entry-level language or as one for large scale development due to its learning curve and inefficiency respectively.Being a student of the Florida State University, I have come to see a great amount of diversity in the subject matter and scope of research projects undertaken by both faculty, affiliated graduate students, and even undergraduates. I wish to conduct a research project of my own regarding the field of Steganography, the process of hiding data amongst other data or within nondescript media. The topic is not currently directly pursued by the faculty of my department, though there are some faculty who may express interest in a more developed project. Contributions for the undertaking of his project such as those of a monetary nature would be used to purchase hardware or time on hardware to run tests on algorithms and methodologies developed. The end product of this project would be the furthering of my own understanding of the material as well as a set of utilities which others interested in the field might use or their own purposes and a software product which a typical person having a moderate familiarity with computers could use to further their own security.I, Preston Hamlin, am a current undergraduate student enrolled in courses under the Computer Science department of the Florida State University, and have been accepted into the Graduate School for the upcoming Fall semester. My interests are diverse, but the strongest ones which I seek to actively pursue are those regarding the graphics pipeline (the series of processes which dynamically take data such as coordinates and textures and produce graphical output) and GPGPU (general purpose GPU programming). The subject of this proposal would fall under the latter. I am well known within my department, and will be continuing on with my studies as a graduate student.As of current, I am good standing with the University. I have worked several terms as a Teaching Assistant for the University through my department. I have conducted myself satisfactorily thus far, and have been granted a teaching assistantship to assist with funding my graduate studies in the terms to come. I am participating in an accelerated program whereby I take graduate courses in lieu of my undergraduate electives, so as to progress towards completion of a MS degree.I actively participate in the research group meetings of one Dr. Mascagni, and have spent said time accruing terminology and outlooks important for his and related areas of academic research.I have made myself rather well known among my peers, Computer Science faculty and staff through my activities in our student chapter of the Association of Computing Machinery (ACM) as well as through the side effects of the aforementioned teaching assistantships. I take an active interest in the growth and development of my department through passive inquiry, participation in the S-STEM program (which aims to promote the STEM fields) and engagement with faculty regarding the nature of departmental advancement.In my time as a Teaching Assistant, I have come to develop a deeper understanding of and appreciation for the significant variance students experience regarding familiarity and competency with computers. Being a rather technically inclined individual, I had taken my understanding and fluency with computational machinery for granted.One major area of divergence is the selection of passwords. As a student of computer science, I am familiar with the methods of storing, checking, and cracking passwords. Humans are not well equipped to remember arbitrary things. A long text password is (in general) more secure than a short one. However, any sufficiently ideal length is well beyond the magical number 7 ± 2 which a human is readily capable of remembering. The typical approach for an individual who wants to generate a longer password than they would otherwise, is to string words together. This provides a sufficient length, as well as assists in the remembering of the password itself, since the person need only remember a handful of concepts which generate the words comprising the password, as opposed to remembering every character independent and irrespective of those around it. However, such passwords are prone to what is called a dictionary-attack. As the name might imply, it consists of selecting words from a list, concatenating them together, and making slight alterations to account for appended symbols or clever misspellings.My proposal, and a slight deviation from my Software Engineering class project, is to use steganography to store (and possibly generate) passwords. Properly implemented, a typical user could utilize an inconspicuous image to contain a password or public/private key. The file could remain on a hard drive, be carried around on a flash drive, or even upload to cloud storage or the Internet at large. Were the file to come into the hands of an unknown or malicious third party (for example if the user’s computer were compromised) the file used to store the password or key would be overlooked without the express knowledge that steganography had been implemented.Public key cryptography is widely used for the identification of individuals or entities on the Internet. It involves the use of a public key, which is publicly distributed, and a private key, which is hidden from all but the owner. To put it simply: the private key is used to encrypt and “sign” a message, while the public key is used to designate a recipient and to decode the message. Were a private key to be stolen, an individual could be impersonated and nefarious deeds may be enacted. Generally, keys are easy to find once a machine is compromised since they are stored in common locations. Were the keys to be stored within inconspicuous files, an attacker (who is likely highly proficient with exploiting security vulnerabilities) may leave the target to probe another potential victim who would seem a more worthwhile mark.The first tangible product of the research project shall be the furthering of my own understanding and body of knowledge.The second shall be a series of methods and algorithms to implement steganography in a general sense. These may very well be reinventing the wheel in the early stages, but are necessary products to exemplify the gained understanding and execution of methodologies. Third, I will develop and implement more complicated algorithms which one might find in a consumer product.Fourth will come a furnished software product which has practical use and monetization potential.Finally, I hope that the publishing of findings and truly unique methods will be achieved in the time spanning the project length. I am currently seeking guidance from faculty within my department so as to asses reasonable values and construct feasible timelines. The typical reasons for a student to undertake the studies for a graduate degree are a yearning for knowledge and a desire to improve upon and apply existing knowledge. These are mine as well. I desire admittance to the university graduate school as well as the program of my own department of Computer Science, so that I might partake in an extended process of higher learning culminating in a Master of Science in Computer Science degree. While the value and importance of the title and certification are not to be understated, I am just as much interested in the knowledge to be attained through studious efforts and relationships with professors. I am currently an undergraduate in good standing with the university and have no outstanding documents or charges. In addition, I have served both my department and the university as a teaching assistant for multiple semesters with satisfactory conduct.I actively participate in the research group meetings of one Dr. Mascagni, and have spent said time accruing terminology and outlooks important for his and related areas of academic research.I have made myself somewhat well known among my peers, Computer Science faculty and staff through my activities in our student chapter of the Association of Computing Machinery (ACM) as well as through the side effects of the aforementioned teaching assistantships.Within my department there is offered a special accelerated program whereby an undergraduate student may substitute graduate courses in favor of their upper-division electives in preparation for completion of a Master of Computer Science degree. Due to complex scheduling problems, if I were to participate in the program I would be unable to graduate with my undergraduate degree until this coming Summer term. As should be clear through the existence of this document and the others correlated with it, I plan to complete a graduate degree. As such, I am participating in the aforementioned program and am currently taking a graduate course with two more to come this Summer term when I graduate.Again I mention the several terms in which I have served as a teaching assistant. While this form of contribution helps to ease the burden on instructors, I seek to assist the university and my department through other means as well. Primarily through engaging in research projects to further the purpose of the university as well as to bolster the outreach and impact of the Computer Science department.The areas which most interest me all lie within the realm of Computer Science. The three areas that outshine the others by a good margin in terms of my level of interest are:Potentially the most visible development over the past decades has been the proliferation of graphical applications and the refinement of methodologies to efficiently process data for rendering to a visual output medium such as a computer monitor. Whereas visual mediums were once highly specific pieces of machinery, they are now much more generic. This lends a certain freedom to focus on the actual content to be displayed, rather than how to display it. As such, many sequential models and methodologies, termed “pipelines,” have come into being and a grand task of achieving efficient and rapid generation of image data has been set forth. The management of pipelines and the dynamic modification of pipeline components to better suit the needs of any given application interests me greatly.Closely related yet significantly different from the above item, General Purpose computing on Graphics Processing Units evolved out of the necessity for effective parallel computation hardware and the prevalence of so-called “graphics cards” which were specially designed hardware for the processing of graphical data. Taking advantage of the overlap of the two markets, modern GPUs usually offer a means of generic coding and processing to offload segments of computation which would be taxing or tedious on a normal CPU but which are effectively independent operations and could instead be executed in parallel.A major utilization of GPGPU is simulation software. That is, software which generates and monitors events for the purposes of imitation, approximation and representation of natural or unnatural phenomena. Such applications typically have vast amounts of data to manage and update, usually on a periodic basis so as to reflect the flow of time. To perform each update in sequence one at a time would be exceptionally inefficient and for the most part prohibit real-time simulation. The usage of GPUs to serve in this capacity makes the accomplishments of tasks in a reasonable timeframe possible, but the generation of the software itself remains a difficult and challenging task.Modern science is concerned with the generation of models and approaches to explain the world around us, as well as to provide insight and predictive measures for future occurrences. Modern applications of artificial intelligence are typically probabilistic decision-making models. These models need to be provided the probability values for certain events or, in the case of machine learning, fed scenarios so as to construct their own sets of values. I am eager to partake in the discussion and development of models and software to predict logical outcomes to scenarios and provide generic decision-making capacities.Several papers by Dr. David Bailey and various co­authors present an analysis of normal numbers and present the possibility of sampling these numbers for the purposes of producing pseudo­random numbers. This is accomplished by the formation of a Linear Congruential Generator via parameterization of a chosen normal number. The task put to this student was to examine existing code for the purposes of development of a generator compatible with the SPRNG library. Normal numbers are rational or irrational numbers whose decimal portions have theproperty that the digits are equidistributed among all the values possible for the base the number is expressed in. For the purposes of this paper, the values referred to as normal are those which satisfy the above criteria and are real. The imaginary normal numbers do not at immediate glance appear to have differing properties which are of interest for the purposes of the generation of random numbers, and indeed seem to only differ in that they possess the requisite imaginary term as a multiplier. A number is deemed to be b­normal for a given base b if every digit composing that base is expressed in the decimal expansion of the normal number exactly as much as every other digit. For example, the digit ‘4’ in base 10 would appear 1/10 of the time, as would ‘6’, ‘5’ and the rest. A base 2 normal number would be composed of ‘0’ and ‘1’, each appearing exactly ½ of the time. In addition, all finite substrings of length k must appear with equal frequency to other substrings of the same length. This can be expressed by stating that all substrings of length K must appear with probability 1/bk. In a 10­normal number, the strings ‘1234’ and ‘4512’ both appear 1/10000 of the time. A number is considered to be absolutely normal if it is b­normal for every base b. It is not outlandish to take issue with the restrictive nature of this definition. As such, another definition is provided. A number is b­dense if every possible finite string of digits is expressed in the number. Now, a number which contains all substrings, but with a slight bias towards a proper subset of the digits the base is expressed in can be easily referred to. From these definitions, it is simple enough to conclude that if a number is b­normal, then it is b­dense. This is because if a number expresses all finite substrings with equal non­zero frequency, then it quite obviously expresses all finite substrings. A number which is b­dense must be irrational. This is due to the fact that rational numbers are inherently periodic, and thus there is an infinite set of strings composed of the digits available to the base which will never be expressed in a rational number. To conclude, those numbers satisfying criteria for naturally equidistributed substrings and which are thus of interest for the purposes of sampling to generate random numbers, are of an irrational nature. Normal Numbers in Nature Several numbers which occur frequently in mathematics and nature are thought to be normal. These include the square root of 2, log10(2), e, and π. There exist several conjectures that these natural constants are indeed irrational. However it can be shown that the number π can be expressed both as a fraction and in a finite number of digits. See Appendix A for details. Opposite these natural constants which are frequented upon in all manner of maths and sciences, are numbers constructed specifically to exhibit exactly those features mentioned in the prior section. One such number, the Champernowne constant, is defined as the number in any base composed of the concatenated integer values in increasing order, expressed in that very same base.  This constant is indeed of interest for many reasons, but sampling this ordered sequence may result in some undesired behavior due to Benford’s law. This number is provably normal for all bases, and should be intuitively expressive of normality. All positive real integers are contained within this constant one after another, and thus all unique combinations of digits for a given base exist within the constant. Other similar constants can be constructed from successive primes, successive Fibonacci numbers, and similar patterns. However, these numbers are all flawed inthat they are not composed of independent (or seemingly independent) values. Two provably normal number generation schemes with a more palatable composition are Stoneham numbers and Korobov numbers. See Appendix B for their formulae.  By generating a normal number in base 2, one essentially has a string of binary digits immediately readable or interpretable as binary data. One can sample just so many bits as desired, interpret as a specific data type, and use that value as a random number. Should the binary stream be sufficiently distributed, any interpretation of sampled bits will tend to be well distributed as well. This is not a guarantee, however. Performing some operation on this raw data may result in skewing or clustering once interpreted. Bailey and Borwein examined a specific Stoneham number of the form α2,3 which is the first (and therefore arguably simplest) of a class of Stoneham numbers. Several similar numbers can be constructed from other satisfactory parameters, such as α2,5. This implies that one can parameterize the inputs to the formulae, resulting in multiple equally valid number streams, which could make for an interesting alternative to leapfrog methods, should multiple streams be desired. The task set to me was to familiarize myself with the papers and produce a modified version of an existing implementation of a Normal LCG. The path was fraught with peril, as my background is significantly lacking in number theory. I started by reviewing the papers, not making much sense of them besides the definitions and conclusions. Changing pace and examining the aforementioned source code, I was met with immediately visible hurdles of double­double arithmetic and natural barriers. I then switched to reviewing the structure and composition of SPRNG. Returning to the papers, I determined that an implementation which parsed the bits as integer values may be more effective and performing than one which casts the values yielded by the existing double­double arithmetic version into integers. The somewhat involved mathematics regarding the properties of normal numbers tooksome time to come to grips with. The symbolism and transitional methods of the referenced papers are understood insofar as the individual processes, but the direction and underlying brilliance behind the navigation through and among representative forms remains elusive. The source code which was provided and made subject to my discerning gaze is that of a functional nature.However, it fell subject to the age­old woes commonly faced when reading another individual’s code. The current states of my modification and implementation are those of an incomplete nature. The current plan of action is to continue development of the code implementing the 128­bit floating point numbers and see its integration into SPRNG with renewed vigor, following a recent clarification of generalized parameters. In the same vein as the potential generation of generators mentioned above, is the possibility of applying generalized or attuned scramblers to these generators so as to further the parallelization capacity of the Normal LCG.Simulations of natural and unnatural events by and large use probabilistic models to determine a course of events which eventually produce output or results of interest. Examples are the modeling of a market economy, particle physics and population fluctuations in a biome. At each point in the simulation, elements are interacting in a known probabilistic manner. Meaning that there is a set of potential actions for each element at each point in time. For example, an atomic particle may continue on its current trajectory or it might suddenly decay and give off radiation which may affect other particles. Sudden economic hardships may impact a production market. A turn in the weather may cause certain flora or fauna to shift in population, impacting the localized food chain.Given that these models are probabilistic in nature, one would desire and expect the occurrence of both simple and complex events to be numerically distributed as per their probabilities. One would have similar expectations that the outcome of one simulation is independent of the outcome of another. Meaning that the paths through the simulations are not pre-determined and are random. To model random events, one needs random numbersThe subject of my research is random numbers, their properties and methods of production. My current item of study is the application of normal numbers as a random number generator, and modifying an existing implementation to produce 64-bit integer output.Normal numbers are numbers which have their digits equidistributed throughout the digits of their base. For example, the number 123456789101112131415… is of interest as any randomly sampled digit or subsequence is just as likely as any other. However, the subsequences are not independent of one another in that they are ordered. There are complicated formulae for deriving these numbers, and I am currently studying them